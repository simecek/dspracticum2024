{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPv6klKyQDnF9aRkNIEjpHz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simecek/dspracticum2024/blob/main/lesson05/Datasets_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HuggingFace `datasets` package\n",
        "\n",
        "Key Features of the `datasets` package:\n",
        " * **Access to a wide variety of datasets**: From standard NLP benchmarks to specialized datasets.\n",
        " * **Efficient data loading and handling**: It uses Apache Arrow under the hood, allowing efficient memory usage and processing speed.\n",
        " * **Integration with Hugging Face Hub**: It allows users to upload and download datasets to/from the Hugging Face Hub.\n",
        " * **Support for streaming datasets**: For very large datasets that may not fit into memory."
      ],
      "metadata": {
        "id": "Cr533nrnp71Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# on colab uncomment this to install the package\n",
        "# !pip install -qq datasets"
      ],
      "metadata": {
        "id": "z8kaZFEsqdD2"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading a dataset\n",
        "\n",
        "The `load_dataset` function is the main entry point to load datasets. You can access a dataset either from the Hugging Face Hub, your local disk, or a remote location.\n",
        "\n",
        "Hereâ€™s an example of loading a dataset of 138,830 arXiv papers converted to multi-markdown (.mmd) format."
      ],
      "metadata": {
        "id": "Tcx-Kebjsfkp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXVCT2VHp7Ll"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"neuralwork/arxiver\")\n",
        "\n",
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds['train']['title'][:5]"
      ],
      "metadata": {
        "id": "c1mgWsOlqkKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds['train'].features"
      ],
      "metadata": {
        "id": "tCUrfQO5uDQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum([\"neural network\" in title.lower() for title in ds['train']['title']])"
      ],
      "metadata": {
        "id": "c14zX3CatCQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accessing the Dataset Splits, Columns and Data Points"
      ],
      "metadata": {
        "id": "3GW_b28uwWz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = ds['train']"
      ],
      "metadata": {
        "id": "UJHru6Asv9fi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.select(range(5))"
      ],
      "metadata": {
        "id": "E-efTBlzwfhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['title'][:5]"
      ],
      "metadata": {
        "id": "Z1TI5zXtwm4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shuffled_train_data = train_data.shuffle(seed=42)\n",
        "shuffled_train_data['title'][:5]"
      ],
      "metadata": {
        "id": "gu7i4Fc0wvns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filtering and Transformations"
      ],
      "metadata": {
        "id": "u5ZlZzUUxQRT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "neural_network_papers = ds['train'].filter(lambda x: 'neural network' in x['title'].lower())"
      ],
      "metadata": {
        "id": "Ur-u3pSSxUx9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neural_network_papers"
      ],
      "metadata": {
        "id": "TOGtVC9Wxn-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function that creates a new column 'title_length'\n",
        "def title_length(x):\n",
        "    return {'title_length': len(x['title'])}"
      ],
      "metadata": {
        "id": "bKQtGz3VxyHv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neural_network_papers.map(title_length)"
      ],
      "metadata": {
        "id": "Uv8TSAE8ybay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Streaming Large Datasets\n",
        "\n",
        "For very large datasets that don't fit in memory, the `datasets` library supports dataset streaming. This loads data in chunks as needed instead of loading everything at once."
      ],
      "metadata": {
        "id": "qAfBS2GjzlSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "streamed_dataset = load_dataset(\"neuralwork/arxiver\", split=\"train\", streaming=True)\n",
        "\n",
        "# Iterate over the first 5 examples in the streamed dataset\n",
        "for i, x in enumerate(streamed_dataset):\n",
        "    if i == 5:\n",
        "        break\n",
        "    print(x)"
      ],
      "metadata": {
        "id": "ndwmWx6GzTl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x['markdown']"
      ],
      "metadata": {
        "id": "YexECbx0z6M4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving and Loading Datasets Locally\n",
        "\n",
        "You can save datasets locally and load them later to avoid redownloading or reprocessing."
      ],
      "metadata": {
        "id": "oiDdOatE0ZxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the tokenized dataset\n",
        "neural_network_papers.save_to_disk(\"neural_network_papers\")\n",
        "\n",
        "# Load the dataset from the saved location\n",
        "from datasets import load_from_disk\n",
        "loaded_dataset = load_from_disk(\"neural_network_papers\")\n",
        "\n",
        "# Check loaded dataset\n",
        "print(loaded_dataset[0])"
      ],
      "metadata": {
        "id": "FV_gZ4qM0BVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uploading to Hugging Face Hub\n"
      ],
      "metadata": {
        "id": "2Bnw-X1K0vrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "neural_network_papers.push_to_hub(\"simecek/neural_network_papers\")"
      ],
      "metadata": {
        "id": "0d0GmaOH0mXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(\"PUT HERE YOUR TOKEN\")"
      ],
      "metadata": {
        "id": "grTqaIJJ2V1F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}