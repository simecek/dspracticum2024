{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5163436-fcca-4ae4-8eaf-6755790380e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run only once\n",
    "!pip install duckduckgo_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8a87803-eb5a-4e72-9724-e86b268b1a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from duckduckgo_search import DDGS\n",
    "\n",
    "def search_images(keyword, max_results=10):\n",
    "    with DDGS() as ddgs:\n",
    "        images = ddgs.images(\n",
    "            keyword,\n",
    "            max_results=max_results\n",
    "        )\n",
    "        return [img['image'] for img in images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77f11575-f03b-4a7f-9144-6e11cc5347df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "418"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword = \"teddybear\"\n",
    "image_urls = search_images(keyword, 1000)\n",
    "len(image_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01aaa328-6d23-4792-976a-e379ad047afe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://i.etsystatic.com/23408950/r/il/4ed2a5/3977829066/il_fullxfull.3977829066_f608.jpg'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_urls[123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c95ce62e-2194-48ff-b7db-d15c335fcbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "import warnings\n",
    "\n",
    "def download_image(url, folder, custom_name=None, verbose=True):\n",
    "    # Create the folder if it doesn't exist\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    # Get the filename from the URL or use the custom name\n",
    "    if custom_name:\n",
    "        filename = custom_name\n",
    "    else:\n",
    "        filename = os.path.basename(urlparse(url).path)\n",
    "        if not filename:\n",
    "            filename = 'image.jpg'  # Default filename if none is found in the URL\n",
    "\n",
    "    # Ensure the filename has an extension\n",
    "    if not os.path.splitext(filename)[1]:\n",
    "        filename += '.jpg'\n",
    "\n",
    "    filepath = os.path.join(folder, filename)\n",
    "\n",
    "    # If the file already exists, append a number to make it unique\n",
    "    base, extension = os.path.splitext(filepath)\n",
    "    counter = 1\n",
    "    while os.path.exists(filepath):\n",
    "        filepath = f\"{base}_{counter}{extension}\"\n",
    "        counter += 1\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the URL with a timeout of 10 seconds\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()  # Raises an HTTPError for bad responses\n",
    "\n",
    "        # Check if the content type is an image\n",
    "        content_type = response.headers.get('content-type', '')\n",
    "        if not content_type.startswith('image'):\n",
    "            if verbose:\n",
    "                warnings.warn(f\"The URL does not point to an image. Content-Type: {content_type}\")\n",
    "            return False\n",
    "\n",
    "        # Write the image content to the file\n",
    "        with open(filepath, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Image successfully downloaded: {filepath}\")\n",
    "        return True\n",
    "\n",
    "    except requests.exceptions.Timeout:\n",
    "        if verbose: \n",
    "            warnings.warn(f\"Download timed out for URL: {url}\")\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        if verbose: \n",
    "            warnings.warn(f\"HTTP error occurred: {e}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        if verbose: \n",
    "            warnings.warn(f\"An error occurred while downloading the image: {e}\")\n",
    "    except IOError as e:\n",
    "        if verbose: \n",
    "            warnings.warn(f\"An error occurred while writing the file: {e}\")\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91e85251-0074-4c49-9118-03c754a02805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc3203363c84d30952e5af7516186d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/418 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "for i, url in enumerate(tqdm(image_urls)):\n",
    "    download_image(url, \"./dataset/teddybear/\", f'image{i}.jpg', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ccf26284-fcab-4fd8-9242-b5b65a9e447a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_urls = search_images('\"black bear\"', 200)\n",
    "len(image_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1bb006d-a6bc-400e-ae61-0d82aa6859ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a633aa40f9104a248697d41728db5cd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, url in enumerate(tqdm(image_urls)):\n",
    "    download_image(url, \"./dataset/blackbear/\", f'image{i}.jpg', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a1dfacf-e854-4244-bfa9-794fbbe12407",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/PIL/Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Training Loss: 0.4643\n",
      "Epoch 1/5, Validation Loss: 0.4380, Validation Accuracy: 0.7419\n",
      "Epoch 2/5, Training Loss: 0.3444\n",
      "Epoch 2/5, Validation Loss: 0.3582, Validation Accuracy: 0.7903\n",
      "Epoch 3/5, Training Loss: 0.2895\n",
      "Epoch 3/5, Validation Loss: 0.3018, Validation Accuracy: 0.8387\n",
      "Epoch 4/5, Training Loss: 0.2166\n",
      "Epoch 4/5, Validation Loss: 0.2398, Validation Accuracy: 0.9274\n",
      "Epoch 5/5, Training Loss: 0.1727\n",
      "Epoch 5/5, Validation Loss: 0.2084, Validation Accuracy: 0.9355\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "dataset = datasets.ImageFolder(root='dataset', transform=transform)\n",
    "\n",
    "# Split into training and validation sets (80% train, 20% validation)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the simple CNN\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # Input channels = 3 (RGB), output channels = 16\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        # Max pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # Fully connected layer (16 * 14 * 14 input features, 2 output classes)\n",
    "        self.fc1 = nn.Linear(16 * 14 * 14, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional layer with ReLU activation and pooling\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # Output: batch_size x 16 x 14 x 14\n",
    "        # Flatten the tensor\n",
    "        x = x.view(-1, 16 * 14 * 14)\n",
    "        # Fully connected layer\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model, define the loss function and optimizer\n",
    "model = SimpleCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    # Calculate average training loss\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}')\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += torch.sum(preds == labels.data)\n",
    "\n",
    "    # Calculate average validation loss and accuracy\n",
    "    val_loss /= len(val_dataset)\n",
    "    val_accuracy = correct.double() / len(val_dataset)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707ed085-6536-489a-a4f5-5625407d0a02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
