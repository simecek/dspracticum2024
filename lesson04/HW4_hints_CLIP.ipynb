{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fad34f7c-9a04-4261-aa2e-f2d5115e9be7",
   "metadata": {},
   "source": [
    "# CLIP embeddings\n",
    "\n",
    "There are models that give you embeeddings both for images and texts. One of them is [CLIP](https://huggingface.co/openai/clip-vit-base-patch32). It is based on a different architecture that DenseNN or CNN that we will discuss later (*transformers*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bf8a74-4e62-4208-8802-a7857668c05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to install transformers\n",
    "# !pip install -qq transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcfac6a-02fb-4687-a8a8-df3b334a5785",
   "metadata": {},
   "source": [
    "## To get embeddings for text/image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49112880-6430-4c2d-a022-08935d3d7c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "def get_clip_embeddings(input_data, input_type='text'):\n",
    "    # Load the CLIP model and processor\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    # Prepare the input based on the type\n",
    "    if input_type == 'text':\n",
    "        inputs = processor(text=input_data, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    elif input_type == 'image':\n",
    "        if isinstance(input_data, str):\n",
    "            image = Image.open(input_data)\n",
    "        elif isinstance(input_data, Image.Image):\n",
    "            image = input_data\n",
    "        else:\n",
    "            raise ValueError(\"For image input, provide either a file path or a PIL Image object\")\n",
    "        inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    else:\n",
    "        raise ValueError(\"Invalid input_type. Choose 'text' or 'image'\")\n",
    "\n",
    "    # Get the embeddings\n",
    "    with torch.no_grad():\n",
    "        if input_type == 'text':\n",
    "            embeddings = model.get_text_features(**inputs)\n",
    "        else:\n",
    "            embeddings = model.get_image_features(**inputs)\n",
    "\n",
    "    return embeddings.numpy()\n",
    "\n",
    "# Example usage\n",
    "text_input = \"A beautiful sunset over the ocean\"\n",
    "text_embedding = get_clip_embeddings(text_input, input_type='text')\n",
    "print(\"Text embedding shape:\", text_embedding.shape)\n",
    "\n",
    "image_path = \"path/to/your/image.jpg\"\n",
    "image_embedding = get_clip_embeddings(image_path, input_type='image')\n",
    "print(\"Image embedding shape:\", image_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a16a15-70f9-4051-8345-93a7bad026b1",
   "metadata": {},
   "source": [
    "## To get embeddings for all images in the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50c0c2a-d2f4-4159-a9ab-69e710dafe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, processor):\n",
    "        self.image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx])\n",
    "        return self.processor(images=image, return_tensors=\"pt\")['pixel_values'][0]\n",
    "\n",
    "def get_clip_embeddings_batch(image_dir, batch_size=32, device='cuda'):\n",
    "    # Load the CLIP model and processor\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    dataset = ImageDataset(image_dir, processor)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "    all_embeddings = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = batch.to(device)\n",
    "            embeddings = model.get_image_features(pixel_values=batch)\n",
    "            all_embeddings.append(embeddings.cpu().numpy())\n",
    "\n",
    "    return np.concatenate(all_embeddings)\n",
    "\n",
    "image_dir = \"path/to/your/folder/\"\n",
    "batch_size = 32  \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "embeddings = get_clip_embeddings_batch(image_dir, batch_size, device)\n",
    "print(\"Embeddings shape:\", embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9adf331-9a32-4eea-9b4e-bda82a9544d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
