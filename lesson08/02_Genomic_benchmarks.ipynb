{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XyJNsvSFac3K"
   },
   "source": [
    "# Train CNN Classifier on human_ocr_ensembl dataset\n",
    "\n",
    "The dataset comes from the [Genomic Benchmarks](https://github.com/ML-Bioinfo-CEITEC/genomic_benchmarks). Best reaults achieved are reported in these [tables](https://github.com/ML-Bioinfo-CEITEC/genomic_benchmarks/tree/main/experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from genomic_benchmarks.data_check import info\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset `human_enhancers_cohn` has 2 classes: negative, positive.\n",
      "\n",
      "All lengths of genomic intervals equals 500.\n",
      "\n",
      "Totally 27791 sequences have been found, 20843 for training and 6948 for testing.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>10422</td>\n",
       "      <td>3474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>10421</td>\n",
       "      <td>3474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          train  test\n",
       "negative  10422  3474\n",
       "positive  10421  3474"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info(\"human_enhancers_cohn\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d01a1e3eedec45dd91ae77e7bab00ec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/477 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration katarinagresova--Genomic_Benchmarks_human_enhancers_cohn-678f4cb48bca8240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None to /home/.cache/huggingface/datasets/katarinagresova___parquet/katarinagresova--Genomic_Benchmarks_human_enhancers_cohn-678f4cb48bca8240/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a1c9305c1464279a49bd6a0770ba95f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60b517ca3d08452282992545cfbb98a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/4.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dee94edb7c3495e9be3ed1692464df5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.66M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff445714f3904abbaec6601f31e4e521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files #1:   0%|          | 0/1 [00:00<?, ?obj/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0e0f142d8d549139893afcd26adc426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files #0:   0%|          | 0/1 [00:00<?, ?obj/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37f7f42a9fa34ae995c50f1ba361b724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/20843 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07f940615d384dbaa6c78045bb80b5ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/6948 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/.cache/huggingface/datasets/katarinagresova___parquet/katarinagresova--Genomic_Benchmarks_human_enhancers_cohn-678f4cb48bca8240/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cd88cfaa9684d6bbf3a57576357a552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"katarinagresova/Genomic_Benchmarks_human_enhancers_cohn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seq': 'TGGTGGTACTTGTCAGGACTTGGAGCAGCAGGTGCAAGATTTAGTGGGTTGGTTTTAGAATATCTGCTTGGAAAGTGGAAAAACTCAATGGATCATCTAGACTTTGGAATTTATCTCCTTCCCCACTTCTCCACTCCCCCAACAACAACAACAACAACAATGACAACAAAAACACCTGGAATAAACAGGTCATACAACGAGGTAGTTGATAGAATAATGTACTTTCCTTTCAGGCACCCCTTGGAGGAGGCAGATTCTGCCCTTTAAGCTGAATCTGCCTTTCCTGCATTTCCTGAAACTCCTGCATTTCCTGAAATCTTCCTGTATTTTCCTGAAATTTCCTGCCATTCCTGAAACTTTAAGGTAACTGTGTCATTAAAGGAAGGAGAGAAGGGAAGTATTAGGACTGCAGATTTGGGGTGCATGATCAGCCTGGCTCTGAGCTTGCAGACTCCCAGAGTCAGGGAAGGGAGGAGCCACCAGCAACCTTGTGGCTTACT',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode and split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(sequence, max_length=500):\n",
    "    one_hot = torch.zeros((4, max_length), dtype=torch.float32)\n",
    "    \n",
    "    mapping = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
    "    \n",
    "    for i, nucleotide in enumerate(sequence[:max_length]):\n",
    "        if nucleotide in mapping:\n",
    "            one_hot[mapping[nucleotide], i] = 1.0\n",
    "\n",
    "    return one_hot\n",
    "    \n",
    "class DNADataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.dataset = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.dataset[idx]['seq']\n",
    "        label = self.dataset[idx]['label']\n",
    "        encoded_seq = one_hot_encode(seq)\n",
    "        return encoded_seq, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = dataset[\"train\"].with_format(\"torch\")\n",
    "ds = DNADataset(ds)\n",
    "\n",
    "train_size = int(0.8 * len(ds))\n",
    "val_size = len(ds) - train_size\n",
    "\n",
    "train_ds, val_ds = torch.utils.data.random_split(ds, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "\n",
    "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple CNN for binary classification of DNA sequences\n",
    "class DNAClassifierCNN(nn.Module):\n",
    "    def __init__(self, kernel_size=5):\n",
    "        super(DNAClassifierCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=4, out_channels=16, kernel_size=kernel_size, stride=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=kernel_size, stride=1)\n",
    "\n",
    "        self.relu = nn.LeakyReLU()        \n",
    "        self.fc1 = nn.Linear(self.count_flatten_size(), 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def count_flatten_size(self):\n",
    "        dummy_input = torch.zeros(1, 4, 500)\n",
    "        dummy_output = self.pool(self.conv2(self.pool(self.conv1((dummy_input)))))\n",
    "        return dummy_output.view(-1).size(0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.reshape(x.size(0), -1)  # Flatten for fully connected layer\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_model(model, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        inputs, labels = batch\n",
    "        labels = labels.float().to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs.to(DEVICE))\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            inputs, labels = batch\n",
    "            labels = labels.float().to(DEVICE)\n",
    "            \n",
    "            outputs = model(inputs.to(DEVICE))\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            total_loss += loss.item()\n",
    "            preds = (outputs.squeeze() > 0.5).float()\n",
    "            correct += (preds == labels).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model training and evaluation after each epoch\n",
    "def evaluation_loop(model, epochs, lr):\n",
    "    \n",
    "    adam = optim.AdamW(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_model(model, train_loader, adam, criterion)\n",
    "        avg_loss, accuracy = evaluate_model(model, val_loader, criterion)\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Validation Loss: {avg_loss}, Accuracy: {accuracy}')\n",
    "    \n",
    "    avg_loss, accuracy = evaluate_model(model, val_loader, criterion)\n",
    "\n",
    "    print(f'Loss: {avg_loss}, Accuracy: {accuracy}\\n')\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Validation Loss: 0.5962875371670905, Accuracy: 0.6764212041256896\n",
      "Epoch 2/5, Validation Loss: 0.5881965183119737, Accuracy: 0.6807387862796834\n",
      "Epoch 3/5, Validation Loss: 0.5886761841883186, Accuracy: 0.6848165027584553\n",
      "Epoch 4/5, Validation Loss: 0.6245531885678531, Accuracy: 0.6449988006716239\n",
      "Epoch 5/5, Validation Loss: 0.6462481829501291, Accuracy: 0.6555528903813864\n",
      "Loss: 0.6462481829501291, Accuracy: 0.6555528903813864\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6555528903813864"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DNAClassifierCNN().to(DEVICE)\n",
    "evaluation_loop(model, epochs=5, lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparam optimization\n",
    "\n",
    "Let's try to optimize the learning rate, number of training epochs and size of the convolution kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    lr = trial.suggest_float('learning_rate', 0.00001, 0.01)\n",
    "    epochs = trial.suggest_int('epochs', 5, 10)\n",
    "    kernel_size = trial.suggest_int('kernel_size', 3, 5)\n",
    "\n",
    "    print(f\"LR: {lr}, Epochs: {epochs}, Kernel size: {kernel_size}\")\n",
    "\n",
    "    model = DNAClassifierCNN(kernel_size=kernel_size).to(DEVICE)\n",
    "\n",
    "    acc = evaluation_loop(model, epochs, lr)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-14 19:19:19,019] A new study created in memory with name: no-name-902363e9-c481-422a-a596-0fd84c9221f4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.008775394190993218, Epochs: 8, Kernel size: 5\n",
      "Epoch 1/8, Validation Loss: 0.5948588561465722, Accuracy: 0.6754617414248021\n",
      "Epoch 2/8, Validation Loss: 0.5907631500531699, Accuracy: 0.6948908611177741\n",
      "Epoch 3/8, Validation Loss: 0.5856786151878707, Accuracy: 0.6869752938354522\n",
      "Epoch 4/8, Validation Loss: 0.5866358623704837, Accuracy: 0.6816982489805709\n",
      "Epoch 5/8, Validation Loss: 0.5891822826771336, Accuracy: 0.6862556968097865\n",
      "Epoch 6/8, Validation Loss: 0.6052291557079054, Accuracy: 0.6816982489805709\n",
      "Epoch 7/8, Validation Loss: 0.6305244030388257, Accuracy: 0.6785799952026865\n",
      "Epoch 8/8, Validation Loss: 0.6614234452029221, Accuracy: 0.6629887263132646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-14 19:25:52,086] Trial 0 finished with value: 0.6629887263132646 and parameters: {'learning_rate': 0.008775394190993218, 'epochs': 8, 'kernel_size': 5}. Best is trial 0 with value: 0.6629887263132646.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6614234452029221, Accuracy: 0.6629887263132646\n",
      "\n",
      "LR: 0.0019691162809866544, Epochs: 10, Kernel size: 5\n",
      "Epoch 1/10, Validation Loss: 0.63472282909255, Accuracy: 0.6543535620052771\n",
      "Epoch 2/10, Validation Loss: 0.5907825411276053, Accuracy: 0.6852962341088991\n",
      "Epoch 3/10, Validation Loss: 0.5995748266008974, Accuracy: 0.6884144878867834\n",
      "Epoch 4/10, Validation Loss: 0.6072489173357724, Accuracy: 0.6713840249460302\n",
      "Epoch 5/10, Validation Loss: 0.616197821520667, Accuracy: 0.6668265771168146\n",
      "Epoch 6/10, Validation Loss: 0.6421644132555896, Accuracy: 0.6639481890141521\n",
      "Epoch 7/10, Validation Loss: 0.7454319737339747, Accuracy: 0.6529143679539458\n",
      "Epoch 8/10, Validation Loss: 0.8840712372583287, Accuracy: 0.6406812185176302\n",
      "Epoch 9/10, Validation Loss: 1.1574187790619508, Accuracy: 0.629407531782202\n",
      "Epoch 10/10, Validation Loss: 1.8198769188109245, Accuracy: 0.6378028304149677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-14 19:34:01,652] Trial 1 finished with value: 0.6378028304149677 and parameters: {'learning_rate': 0.0019691162809866544, 'epochs': 10, 'kernel_size': 5}. Best is trial 0 with value: 0.6629887263132646.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.8198769188109245, Accuracy: 0.6378028304149677\n",
      "\n",
      "LR: 0.002985461977556319, Epochs: 10, Kernel size: 5\n",
      "Epoch 1/10, Validation Loss: 0.594495553779238, Accuracy: 0.6742624130486927\n",
      "Epoch 2/10, Validation Loss: 0.5822400102633556, Accuracy: 0.6905732789637803\n",
      "Epoch 3/10, Validation Loss: 0.5824706706836933, Accuracy: 0.6920124730151115\n",
      "Epoch 4/10, Validation Loss: 0.5930814285769718, Accuracy: 0.6934516670664428\n",
      "Epoch 5/10, Validation Loss: 0.5961155054223446, Accuracy: 0.6812185176301271\n",
      "Epoch 6/10, Validation Loss: 0.6533085828974047, Accuracy: 0.6656272487407052\n",
      "Epoch 7/10, Validation Loss: 0.7593945276191216, Accuracy: 0.6627488606380427\n",
      "Epoch 8/10, Validation Loss: 1.126364355323879, Accuracy: 0.6502758455265052\n",
      "Epoch 9/10, Validation Loss: 1.2706463275519946, Accuracy: 0.6339649796114176\n",
      "Epoch 10/10, Validation Loss: 1.5852241702662169, Accuracy: 0.6406812185176302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-14 19:42:08,979] Trial 2 finished with value: 0.6406812185176302 and parameters: {'learning_rate': 0.002985461977556319, 'epochs': 10, 'kernel_size': 5}. Best is trial 0 with value: 0.6629887263132646.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.5852241702662169, Accuracy: 0.6406812185176302\n",
      "\n",
      "LR: 0.0026759141652755746, Epochs: 10, Kernel size: 3\n",
      "Epoch 1/10, Validation Loss: 0.6024388545797071, Accuracy: 0.6637083233389302\n",
      "Epoch 2/10, Validation Loss: 0.5963589330665938, Accuracy: 0.666106980091149\n",
      "Epoch 3/10, Validation Loss: 0.6459926032383023, Accuracy: 0.6548332933557208\n",
      "Epoch 4/10, Validation Loss: 0.6000486842093576, Accuracy: 0.673782681698249\n",
      "Epoch 5/10, Validation Loss: 0.6373377804082768, Accuracy: 0.662269129287599\n",
      "Epoch 6/10, Validation Loss: 0.6635607164779692, Accuracy: 0.6577116814583833\n",
      "Epoch 7/10, Validation Loss: 0.7803482347317324, Accuracy: 0.6471575917486208\n",
      "Epoch 8/10, Validation Loss: 0.8862832798302629, Accuracy: 0.6452386663468458\n",
      "Epoch 9/10, Validation Loss: 1.1663809441428148, Accuracy: 0.633245382585752\n",
      "Epoch 10/10, Validation Loss: 1.4030394840786475, Accuracy: 0.6315663228591989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-14 19:50:16,491] Trial 3 finished with value: 0.6315663228591989 and parameters: {'learning_rate': 0.0026759141652755746, 'epochs': 10, 'kernel_size': 3}. Best is trial 0 with value: 0.6629887263132646.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.4030394840786475, Accuracy: 0.6315663228591989\n",
      "\n",
      "LR: 0.008592914787533152, Epochs: 8, Kernel size: 4\n",
      "Epoch 1/8, Validation Loss: 0.6934835692398421, Accuracy: 0.49796114176061407\n",
      "Epoch 2/8, Validation Loss: 0.6931513379548342, Accuracy: 0.49796114176061407\n",
      "Epoch 3/8, Validation Loss: 0.6931708172987435, Accuracy: 0.502038858239386\n",
      "Epoch 4/8, Validation Loss: 0.6933394183639352, Accuracy: 0.502038858239386\n",
      "Epoch 5/8, Validation Loss: 0.6935275451827595, Accuracy: 0.49796114176061407\n",
      "Epoch 6/8, Validation Loss: 0.6934432792299576, Accuracy: 0.502038858239386\n",
      "Epoch 7/8, Validation Loss: 0.69342215689084, Accuracy: 0.502038858239386\n",
      "Epoch 8/8, Validation Loss: 0.6931612737306202, Accuracy: 0.49796114176061407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-14 19:56:50,487] Trial 4 finished with value: 0.49796114176061407 and parameters: {'learning_rate': 0.008592914787533152, 'epochs': 8, 'kernel_size': 4}. Best is trial 0 with value: 0.6629887263132646.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6931612737306202, Accuracy: 0.49796114176061407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'learning_rate': 0.008775394190993218, 'epochs': 8, 'kernel_size': 5}\n",
      "Best value (validation AU PRC): 0.6629887263132646\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best hyperparameters: {study.best_params}\")\n",
    "print(f\"Best value (validation AU PRC): {study.best_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "environment": {
   "name": "pytorch-gpu.1-9.m75",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m75"
  },
  "interpreter": {
   "hash": "af167c304fdc99884e31a029277e630a5b00036f91292350fffdeed1d15b16ff"
  },
  "kernelspec": {
   "display_name": "Python [conda env:transformers]",
   "language": "python",
   "name": "conda-env-transformers-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
