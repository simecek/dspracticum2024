{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9ac2d70-6791-4b1f-b65d-b8c88ce0eeba",
   "metadata": {},
   "source": [
    "# Train your own small GPT-2 model\n",
    "\n",
    "If you want to experiment with the trained model, you can do it at `Inference API` panel of\n",
    "\n",
    "https://huggingface.co/openai-community/gpt2?text=My+name+is+Thomas+and+my+main\n",
    "\n",
    "Note that we are training small GPT2 model on a tiny dataset. Still We can see observe how the model improve with the number of steps and get some interesting results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4ce989-ea7f-4c89-975c-046038996d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import wandb  # we will talk about wandb next lecture\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef7cfa7-f664-4ea9-8f04-17ce270272de",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "\n",
    "Before training, we have to tokenize the data and split them into chunks of the same size as context size of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec1afff-194b-461c-8073-75b769ccc0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your own dataset\n",
    "dataset = load_dataset(\"simecek/wikipedie_20230601\")\n",
    "\n",
    "# Make validation split\n",
    "dataset = dataset['train'].train_test_split(test_size=0.0015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3e836a-8548-496b-bdaf-d2b98f5e2596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the gpt-2 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token=tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b1dfeb-3191-414e-b6b9-0b198946d334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the dataset\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(text=example[\"text\"])\n",
    "tokenized_ds = dataset.map(tokenize_function, batched=True, remove_columns='text')\n",
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cdb38c-eb4f-47e7-8c43-96bacb7826ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "def concatenate_and_chunk(dataset, chunk_size=512):\n",
    "    # Flatten all `input_ids` into a single list\n",
    "    all_input_ids = list(chain(*dataset[\"input_ids\"]))\n",
    "    \n",
    "    # Create chunks of `chunk_size`\n",
    "    chunks = [all_input_ids[i:i + chunk_size] for i in range(0, len(all_input_ids), chunk_size)]\n",
    "    \n",
    "    # Only keep chunks that are exactly of length `chunk_size`\n",
    "    chunks = [chunk for chunk in chunks if len(chunk) == chunk_size]\n",
    "    \n",
    "    # Create a new dataset with only the `input_ids` chunks\n",
    "    return Dataset.from_dict({\"input_ids\": chunks})\n",
    "\n",
    "# Apply this function to each split (train and test) in the DatasetDict\n",
    "chunked_ds = DatasetDict({\n",
    "    split: concatenate_and_chunk(split_ds, chunk_size=512)\n",
    "    for split, split_ds in tokenized_ds.items()\n",
    "})\n",
    "\n",
    "chunked_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f1ef42-f488-4ca0-af36-464f3e535be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data collator joins chunks into batches\n",
    "# see https://huggingface.co/docs/transformers/en/main_classes/data_collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe26fe6-925f-49ec-a854-ebdb429c66ad",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37301b62-4bea-4006-aa1a-7785924227b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model configuration for the smallest GPT-2\n",
    "config = GPT2Config(\n",
    "    vocab_size=len(tokenizer),      # Standard GPT-2 vocab size 50257\n",
    "    n_positions=512,                # Context size (512 is enough for small-scale models)\n",
    "    n_embd=768,                     # Embedding size\n",
    "    n_layer=12,                     # Number of transformer layers\n",
    "    n_head=12,                      # Number of attention heads\n",
    ")\n",
    "\n",
    "# Initialize the model and tokenizer\n",
    "model = GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a48b42-9ce2-433f-acff-ce694449c317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "# Define the perplexity metric\n",
    "def compute_metrics(eval_pred):\n",
    "    # `eval_pred` is a tuple of (logits, labels)\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    # Convert logits and labels to PyTorch tensors if they are NumPy arrays\n",
    "    if isinstance(logits, np.ndarray):\n",
    "        logits = torch.tensor(logits)\n",
    "    if isinstance(labels, np.ndarray):\n",
    "        labels = torch.tensor(labels)\n",
    "\n",
    "    # Shift labels so that tokens align for calculating loss\n",
    "    shift_labels = labels[:, 1:].reshape(-1)\n",
    "    shift_logits = logits[:, :-1, :].reshape(-1, logits.shape[-1])\n",
    "\n",
    "    # Calculate the cross-entropy loss\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)  # Ignore padding tokens\n",
    "    loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "    # Calculate perplexity\n",
    "    perplexity = math.exp(loss.item())\n",
    "    return {\"perplexity\": perplexity}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff4c203-9d5d-494e-b425-4b5a265516ff",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ff634b-e879-44ad-a6d1-21c0a573aa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this according to size of your dataset\n",
    "# You should train for at least 15 mins on A10 GPU to get something reasonable\n",
    "TRAIN_EPOCHS = 10\n",
    "\n",
    "SAVE_STEPS = 1000\n",
    "EVAL_STEPS = SAVE_STEPS // 2\n",
    "\n",
    "# training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-training\",  # Directory to save the model checkpoints and other outputs\n",
    "    eval_strategy=\"steps\",  # Evaluation strategy to use during training ('steps' or 'epochs')\n",
    "    eval_steps=EVAL_STEPS,  # Perform evaluation every 500 steps\n",
    "    num_train_epochs=TRAIN_EPOCHS,  # Total number of training epochs\n",
    "    per_device_train_batch_size=16,  # Batch size for training on each device\n",
    "    per_device_eval_batch_size=16,  # Batch size for evaluation on each device\n",
    "    learning_rate=2.5e-4,  # Initial learning rate for the optimizer\n",
    "    lr_scheduler_type='cosine',  # Learning rate scheduler type. 'cosine' provides a cosine decay schedule.\n",
    "    warmup_ratio=0.05,  # Proportion of training to perform linear learning rate warmup for\n",
    "    adam_beta1=0.9,  # Beta1 parameter for the Adam optimizer (first moment decay)\n",
    "    adam_beta2=0.999,  # Beta2 parameter for the Adam optimizer (second moment decay)\n",
    "    weight_decay=0.01,  # Weight decay to apply (L2 regularization)\n",
    "    logging_strategy=\"steps\",  # Logging strategy to use. 'steps' logs at specified steps.\n",
    "    logging_steps=EVAL_STEPS,  # Log training metrics every 500 steps\n",
    "    save_steps=SAVE_STEPS,  # Save a checkpoint every 1000 steps\n",
    "    save_total_limit=10,  # Maximum number of checkpoints to keep. Older checkpoints are deleted.\n",
    "    # report_to='wandb',  # Uncomment to report metrics to Weights and Biases (optional)\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                 args = training_args,\n",
    "                 tokenizer=tokenizer,\n",
    "                 train_dataset=chunked_ds[\"train\"],\n",
    "                 eval_dataset=chunked_ds[\"test\"],\n",
    "                 compute_metrics=compute_metrics,\n",
    "                 data_collator = data_collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72a95fd-d7ac-4403-81d8-a42532f7a7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b62744-896e-4faf-b21b-3e031dc1217d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./gpt2-small-final\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01639456-2124-46e8-8d6c-46b4cbd5c484",
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_MODEL_NAME = \"my_small_gpt2_cswiki\" # change this\n",
    "HF_TOKEN = \"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\"  # change this \n",
    "\n",
    "model.push_to_hub(YOUR_MODEL_NAME, token=HF_TOKEN)\n",
    "tokenizer.push_to_hub(YOUR_MODEL_NAME, token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd6dac4-f46e-42a2-833f-07cb16eab3bc",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Now you can switch from GPU to CPU. Try to complete some prompt specific to your dataset.\n",
    "\n",
    "Does it make sense? Is it at least in Czech/Slovak?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9ade4a-f8b3-441d-8b6f-550048a21f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  GPT2LMHeadModel, AutoTokenizer, pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token=tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ee0298-2d7f-4aa4-9a2d-13429b7e197e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  GPT2LMHeadModel.from_pretrained(\"./gpt2-small-final\")\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42697e6-10ad-4ea8-a9a9-addef655a1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"Vlak jede\" # Set starting prompt, something specific for your dataset\n",
    "\n",
    "generator(\n",
    "    PROMPT,\n",
    "    max_length=50,       # Maximum length of the generated text\n",
    "    do_sample=True,\n",
    "    temperature=0.5,         # Experiment with this\n",
    "    repetition_penalty=1.9,  # Experiment with this\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b12a36-09d5-4e3b-99bd-1ea157012107",
   "metadata": {},
   "source": [
    "Now go back to your training folder `.gpt2-training/`. Each `checkpoint-N` folder contains the model saved after N steps. \n",
    "\n",
    "If you experiment with the older models, you should see that the models improves with time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7a20e6-c47b-44e5-8d1a-2e6825dbef74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_after_N_steps(N, prompt, **kwargs):\n",
    "    model =  GPT2LMHeadModel.from_pretrained(f\"./gpt2-training/checkpoint-{N}/\")\n",
    "    generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "    output = generator(prompt, **kwargs)\n",
    "    return output  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc850f1-9c94-4a71-b56b-a2eecc5eecf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sample_after_N_steps(1000, \"Pokus\", do_sample=True, temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035fbb7a-4147-435f-b9fc-f3c30a6eea1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sample_after_N_steps(2000, \"Pokus\", do_sample=True, temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64f2fe5-9ada-497b-aff0-10b0bb327785",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sample_after_N_steps(3000, \"Pokus\", do_sample=True, temperature=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
