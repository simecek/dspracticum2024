**Date**: Nov 4, 2024

**Studio**: https://lightning.ai/simecek/studios/lesson-06~01hv3p406bs3pebc3xm2qwtwz1/

**Slides**: https://docs.google.com/presentation/d/1T6IR09LgYzZArSy8QfHkOmlTKOre3NJjrzzWerX3aRA/edit?usp=sharing

**Videos**:

* [3Blue1Brown: How large language models work, a visual intro to transformers](https://www.youtube.com/watch?v=wjZofJX0v4M&ab_channel=3Blue1Brown)
* [3Blue1Brown: Attention in transformers, visually explained](https://www.youtube.com/watch?v=eMlx5fFNoYc&ab_channel=3Blue1Brown)
* [Andrej Karpathy: Let's build GPT: from scratch, in code, spelled out](https://www.youtube.com/watch?v=kCc8FmEb1nY)
* [Andrej Karpathy: Let's reproduce GPT-2 (124M)](https://www.youtube.com/watch?v=l8pRSuU81PU)
* [Stanford: Building Large Language Models (LLMs)](https://www.youtube.com/watch?v=9vM4p9NN0Ts)

**Papers**:

* [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
* [BERT paper (Pre-training of Deep Bidirectional Transformers for Language Understanding)](https://arxiv.org/abs/1810.04805)
* [GPT-2 paper (Language Models are Unsupervised Multitask Learners)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
* [GPT-3 paper (Language Models are Few-Shot Learners)](https://arxiv.org/abs/2005.14165)

**Assignment 06** (due to Nov 11, 8:30 AM):

t.b.d.


