{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d253cf9b-d1df-497f-b4da-0198b09eee13",
   "metadata": {},
   "source": [
    "# HW03 Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca75edbb-453d-4fc2-9815-e47060acacaa",
   "metadata": {},
   "source": [
    "I recommend to work in [Lightning studio](https://lightning.ai/simecek/studios/fastai-transfer-learning-fine-tuning~01hv3p406bs3pebc3xm2qwtwz1) (and not Colab), because of preinstalled environment and permanent disk space.\n",
    "\n",
    "## Data preparation\n",
    "\n",
    " 1. You need to upload your dataset to the virtual machine. Since the dataset is relatively small, it's easiest to ZIP the data and use the 'Upload Files' button in the top left corner.\n",
    " 2. You can unzip the data either through the Terminal or directly within the Jupyter Notebook using the command: `!unzip YOURFILENAME.zip`\n",
    " 3. Optionally, organize the image files as shown in the schema below.\n",
    "\n",
    "```\n",
    "DATASET_NAME/\n",
    "│\n",
    "├── train/\n",
    "│   ├── CATEGORY1/\n",
    "│   │   ├── image1.jpg\n",
    "│   │   ├── image2.jpg\n",
    "│   │   └── ...\n",
    "│   ├── CATEGORY2/\n",
    "│   │   ├── image1.jpg\n",
    "│   │   ├── image2.jpg\n",
    "│   │   └── ...\n",
    "│   └── ...\n",
    "│\n",
    "└── test/\n",
    "    ├── CATEGORY1/\n",
    "    │   ├── image1.jpg\n",
    "    │   ├── image2.jpg\n",
    "    │   └── ...\n",
    "    ├── CATEGORY2/\n",
    "    │   ├── image1.jpg\n",
    "    │   ├── image2.jpg\n",
    "    │   └── ...\n",
    "    └── ...\n",
    "```\n",
    "\n",
    "If the data are organized in this form, you can obtain dataloaders as below. Otherwise, adapt `ImageDataLoaders` / `DataBlocks` accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51471823-d010-4db1-b842-25768d1b4732",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from pathlib import Path\n",
    "\n",
    "# TODO - change the path!\n",
    "path = Path('/path/to/your/dataset') / 'train'\n",
    "\n",
    "dls = ImageDataLoaders.from_folder(\n",
    "    path,                 # Path to the train folder\n",
    "    train='.',            # Use the entire train folder for splitting\n",
    "    valid_pct=0.2,        # 20% of the data will be used for validation\n",
    "    seed=42,              # Set a seed for reproducibility\n",
    "    item_tfms=Resize(460),# Resize the images to 460x460 (or your desired size)\n",
    "    batch_tfms=aug_transforms(size=224, min_scale=0.75), # Data augmentation \n",
    "    bs=64                 # Batch size 64\n",
    ")\n",
    "\n",
    "dls.show_batch(max_n=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e3b641-cd96-4adb-a968-dfc0e0c1d912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# to ignore the palette warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"Palette images with Transparency expressed in bytes should be converted to RGBA images\",\n",
    "    category=UserWarning\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efd3d08-9c1d-4370-85ec-ed28ccd05068",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "  1. Choose a pretrained model.\n",
    "  2. Determine the optimal learning rate using the learning rate finder.\n",
    "  3. Fine-tune the pretrained model to adapt it to your dataset.\n",
    "\n",
    "Regarding the model, use rather [something smaller](https://www.kaggle.com/code/jhoward/which-image-models-are-best) from `timm` package. Architectures like convexnet, resnet, vgg or mobilenet should be fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352fe555-38c2-4581-b2b8-cbfcd644ff75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - you can change resnet18 to other architecture\n",
    "# from timm import list_models\n",
    "# list_models(pretrained=True)\n",
    "learn = vision_learner(dls, resnet18, metrics=error_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82e14ca-b197-4f7d-a901-94dadbe1437c",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a115b0-db44-44eb-9991-2a2ac5b59ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "lr =  # set optimal learning rate here\n",
    "epochs =  # set number of one-cycles for training, try to experiment a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f12eb49-50cd-413d-8bd8-a6c96515ea69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - you can change resnet18 to other architecture (same as above)\n",
    "learn = vision_learner(dls, resnet18, metrics=error_rate)\n",
    "learn.fine_tune(epochs, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fce2d60-3913-4087-85b5-5e8b3fb6a0ec",
   "metadata": {},
   "source": [
    "Evaluate the model's performance..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deef903-2409-4ecd-9467-0e5587c16bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "interp.plot_confusion_matrix(figsize=(8,8), dpi=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6169a6d4-7440-4950-a3d8-65f8dec4a8f2",
   "metadata": {},
   "source": [
    "...and see the misclassified images in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903c4e07-3724-457d-bbe7-e27ec0fbe686",
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.plot_top_losses(9, figsize=(13,13))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97aacc62-482d-4690-82fb-e913325cb467",
   "metadata": {},
   "source": [
    "If you see a mess, you might want to clean your data - there is a widget for that! It will list the most suspicios images (=highest loss) per set (train/valid) and category. Mark these images for deletion..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45971c6e-af17-4cf9-a643-e8b75244df56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.widgets import *\n",
    "\n",
    "# TODO - repeatedly run this and following cell for each category and split (train/valid)\n",
    "cleaner = ImageClassifierCleaner(learn)\n",
    "cleaner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057de41c-1b17-497c-adfa-841e9941fbd8",
   "metadata": {},
   "source": [
    "...and now actually delete them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d2c00e-ef7b-46dd-857c-d9eee82d8d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in cleaner.delete(): cleaner.fns[idx].unlink()\n",
    "print(len(cleaner.delete()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16886368-9376-496d-8f89-2cb48a064c58",
   "metadata": {},
   "source": [
    "After deleting the images, you need to rerun the entire notebook from the first cell to update the dataloaders, as they will point to deleted files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b12d4c6-cd6f-4f96-ab3f-714956b5ffb7",
   "metadata": {},
   "source": [
    "## Test the model on a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639b6ae9-c79c-4459-b312-32b677211966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test DataLoader\n",
    "test_path = Path('### PATH TO YOUR DATASET FOLDER ###') / 'test'\n",
    "test_dl = learn.dls.test_dl(get_image_files(test_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e093997-6c56-45db-afc8-2e24ec287ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "preds, targets = learn.get_preds(dl=test_dl)\n",
    "pred_labels = torch.argmax(preds, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a636cf-4ba1-4061-b950-a34c936a91bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class names\n",
    "class_names = learn.dls.vocab\n",
    "# Convert predictions to class names\n",
    "pred_class_names = [class_names[i] for i in pred_labels]\n",
    "true_class_names = [Path(t).parent.name for t in test_dl.items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee5bd35-5291-43b3-bc27-f46ac59e8688",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "print(\"Test error rate:\")\n",
    "print(1 - mean([actual == predicted for actual, predicted in zip(true_class_names, pred_class_names)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebe866b-ab17-46c7-8203-6c35a5554803",
   "metadata": {},
   "source": [
    "## Let us save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef688e5-b5e4-460b-afc3-276d83199eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export('model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf011ac-0415-4505-a5e2-8324f4fe0818",
   "metadata": {},
   "source": [
    "Make sure to **download** `model.pkl` to your laptop, as you will need it later."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
